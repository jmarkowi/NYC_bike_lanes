{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into `train`/`test` folders\n",
    "\n",
    "Using split-folders package: https://pypi.org/project/split-folders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T17:41:20.461620Z",
     "start_time": "2021-10-19T17:41:20.406462Z"
    }
   },
   "outputs": [],
   "source": [
    "import splitfolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From instructions on package's website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:42:30.666085Z",
     "start_time": "2021-10-13T14:42:30.663975Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Split with a ratio.\n",
    "# # To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "# splitfolders.ratio(\"raw_images\", \n",
    "#                    output=\"input_images\", \n",
    "#                    seed=42, \n",
    "#                    ratio=(.8, .2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tues 10/12, 1:30pm:**\n",
    "\n",
    "It worked! The package created copies of all my files, sorted into `train` and `val` folders, each containing the originally named folders/split. \n",
    "\n",
    "For now I'm going to delete these, then re-run this code after I have added to the dataset a little today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:43:35.890704Z",
     "start_time": "2021-10-13T14:43:19.988663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1461 files [00:15, 91.90 files/s] \n"
     ]
    }
   ],
   "source": [
    "# Split with a fixed number of items.\n",
    "# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.\n",
    "splitfolders.fixed(\"raw_images\", \n",
    "                   output=\"input_images\", \n",
    "                   seed=42, \n",
    "                   fixed=(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wed 10/13, 10:30am:**\n",
    "\n",
    "Performed operation on full dataset after cropping images.\n",
    "\n",
    "Decided to set aside 100 images of each class as holdout/test set and will use `validation_split` parameter in Keras's `ImageDataGenerator` to split the training set during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2:30pm:**\n",
    "\n",
    "Had to repeat this process after re-orienting all images with `exif_transpose` (see EDA notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:36:00.309284Z",
     "start_time": "2021-10-13T18:36:00.137457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete metadata files created by Mac OS\n",
    "!find . -name \".DS_Store\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T18:36:10.807155Z",
     "start_time": "2021-10-13T18:36:01.910381Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1461 files [00:08, 164.31 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Split with a fixed number of items.\n",
    "# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.\n",
    "splitfolders.fixed(\"processed_images\", \n",
    "                   output=\"input_images\", \n",
    "                   seed=42, \n",
    "                   fixed=(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thu Oct 14, 12pm:**\n",
    "\n",
    "At data meeting with Max, he suggested reducing the number in the holdout set to 100 total (not 100 per class, which tbh I thought I was doing). Maximize training set since it's already so small!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T17:12:32.371412Z",
     "start_time": "2021-10-14T17:12:18.723115Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1461 files [00:13, 108.61 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Delete metadata files created by Mac OS\n",
    "!find . -name \".DS_Store\" -delete\n",
    "\n",
    "# Split with a fixed number of items.\n",
    "# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.\n",
    "splitfolders.fixed(\"other_images/processed_images\", \n",
    "                   output=\"input_images\", \n",
    "                   seed=42, \n",
    "                   fixed=(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tues Oct 19, 1:30pm:**\n",
    "\n",
    "Realized that when I use `validation_split=0.1` in an `ImageDataGenerator` during training, it just takes the last 1% of images *in order* from the training class folders!!! This is a seriously huge issue since my data isn't stored in a random order. In fact, that means that the Google Maps Street View screenshots, which are last alphabetically, are only ever used for validation and never for training! Ultimately this means that my models are being trained and validation on a non-random sample of my dataset.\n",
    "\n",
    "To address this issue, I will use `split-folders` to create a separate `validation` folder of images to use during training via a separate `ImageDataGenerator` specifically for that purpose, instead of using the `validation_split` parameter. Since `split-folders` does shuffle data during assignment, it should take care of this issue. I probably should have done this in the first place, but thought that using `validation_split` was better.\n",
    "\n",
    "Not only that, but apparently using `validation_split` with image augmentation means that the validation set is still being augmented! This is seriously not good since *no* holdout/test set images (or new images being used for prediction) will be augmented. It means that ultimately my model validation metrics are not fully valid since they are a non-random sample *and* they've been augmented with image augmentation.\n",
    "\n",
    "Creating a new folder of images specifically for validation will take care of both of these issues, although it still means that my original test/holdout set (which I won't touch or add to) will not include new data/images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T17:41:38.690591Z",
     "start_time": "2021-10-19T17:41:22.557939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1719 files [00:15, 107.95 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Delete metadata files created by Mac OS\n",
    "!find . -name \".DS_Store\" -delete\n",
    "\n",
    "# Split with a fixed number of items.\n",
    "# To only split into training and validation set, use a single number to `fixed`, i.e., `10`.\n",
    "splitfolders.fixed(\"input_images/full_combined\", \n",
    "                   output=\"input_images/validation\", \n",
    "                   seed=42, \n",
    "                   fixed=(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

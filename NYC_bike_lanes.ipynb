{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stay In Your Lane!\n",
    "## Automated Bike Lane Enforcement With Neural Network Image Classification: Technical Notebook\n",
    "### Author: Jesse Markowitz, October 2021\n",
    "\n",
    "<img src=\"readme_images/cab_in_bikelane.png\" alt=\"a scene often seen in NYC\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T14:32:04.416920Z",
     "start_time": "2021-10-20T14:31:57.650018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from functions import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biking is my primary mode of transportation in and around New York City, as it is for an increasing number of people every year. I bike to commute to work, for groceries or other personal trips, and for exercise/pleasure. However, on every single trip I make in the city, I face a serious safety issue: **cars parked in bike lanes force me to weave in and out of traffic.** Although it is illegal to stop, stand, or park in a bike lane, vehicles in the city frequently do. Despite an increase in bike infrastructure and ridership in NYC, this problem continues, seemingly unenforced and unabated. The worst offenders are Taxi and Limousine Commission (T&LC) cars (yellow and green cabs, as well as rideshare vehicles for Uber, Lyft, etc.), delivery trucks, and police vehicles (personal and service). While it is possible to report offenders via 311 (the [Reported app](https://reportedly.weebly.com/) makes this especially easy), in general it is only T&LC drivers who are held accountable for these violations, as there is a set of prosecutors specifically for that regulatory purpose. When it comes to personal and police vehicles, 311 forwards the complaint to the local police precinct, where it is up to the responding officer's discretion to follow up. This rarely occurs. **Insufficient enforcement of bike lane traffic laws creates serious safety issues for cyclists.**\n",
    "\n",
    "<img src=\"readme_images/blocked_bike_lane_nj_port_authority.png\" alt=\"yet another cop in the bike lane\" width=\"600\"/>\n",
    "\n",
    "On September 15, 2021, the NYC DOT released a [\"Request for Expressions of Interest\"](https://a856-cityrecord.nyc.gov/RequestDetail/20210907107) to create a system for automated bike lane enforcement. A system for bus lanes called the Automated Bus Lane Enforcement (ABLE) system was created by [Siemens Mobility](https://www.mobility.siemens.com/us/en/company/newsroom/short-news/first-ever-mobile-bus-lane-enforcement-solution-in-new-york.html) and installed in 2010 and has been expanded since then with great success, as measured by increased route speed and ridership. Automating enforcement of bike lane traffic laws would have the immediate effect of increase enforcement from what seem to be negligible levels. Automated enforcement would also benefit the city's cyclists by reducing the need for active police involvement with the issue, especially on streets where the problem is the greatest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of just over 1,800 images of New York City bike lanes, up from about 1,600 at the beginning of the project. Just over half of these images are of a bike lane obstructed by a vehicle, which comprises the target class. The rest of the images are of bike lanes without vehicular obstruction, showing entirely empty bike lanes or, on occassion, bike lanes with cyclists or pedestrians. The small size of the dataset is one of the most significant limitations of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images in the dataset were collected from a variety of sources:\n",
    " - The [Reported app's Twitter page](https://twitter.com/Reported_NYC), which tweets all traffic violations reported through the app\n",
    " - A large dataset of images provided by [Ryan Gravener](https://github.com/snooplsm), who is working on an image recognition project for Reported\n",
    " - Screenshots from Google Maps Street View\n",
    " - Manual collection (i.e., taking photos while biking around the city--this is the source of the vast majority of the non-target images of unobstructed bike lanes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-orienting images\n",
    "All of the images collected manually were taken with an iPhone X and saved as jpegs. Because we want to display images oriented correctly, digital cameras and smartphone cameras attach an Orientation tag to the EXIF data with each photo taken. This tag is read by most image display programs in order to orient the image correctly without altering the underlying image data, but a Keras `ImageDataGenerator` does not do this. As a result, the raw images collected manually are improperly oriented when fed into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping images\n",
    "Many of the images collected via Reported contain timestamps printed at the top of the image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T15:12:06.730089Z",
     "start_time": "2021-10-20T15:12:06.575675Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXAMPLE OF TIMESTAMP IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to enhance the photo's value as evidence, but creates a potentially confounding factor in the dataset because images with timestamps will be overrepresented in the target class. Without removing this feature, it's possible that the model will use it to predict the target class, rather than attending to real features in the image.\n",
    "\n",
    "Cropping the top of the image is an easy way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T14:32:08.712749Z",
     "start_time": "2021-10-20T14:32:08.648346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_images/full_combined/open_bike_lane'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filepaths\n",
    "train_dir = 'input_images/full_combined'\n",
    "\n",
    "train_open_dir = os.path.join(train_dir, 'open_bike_lane')\n",
    "train_vehicle_dir = os.path.join(train_dir, 'vehicle_bike_lane')\n",
    "\n",
    "train_open_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing unclear images\n",
    "The final step before a train-val-test split is to manually review images, ensuring they are in the correct class directory, and removing any that are inappropriate or unclean. Images were generally removed that:\n",
    " - did not show both lane lines of a bike lane or were too \"close up\" to a vehicle\n",
    " - contained too many cyclists, motorbikes, or pedestrians such that the bike lane was significantly obstructed\n",
    " - were taken at night (these were extremely overrepresented in the target class)\n",
    " - showed a car crossing a bike lane legally (i.e., crossing an intersection)\n",
    " - were deemed to not adequately contain the information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T15:20:40.712797Z",
     "start_time": "2021-10-20T15:20:40.654787Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXAMPLES OF UNUSED IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these decisions were subjective judgments and there were a surprisingly large number of images that were ambiguous. These images were kept in a separate `unused_images` folder for later inclusion or testing and as non-examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T14:32:25.599072Z",
     "start_time": "2021-10-20T14:32:25.231122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Delete metadata files created by Mac OS\n",
    "!find . -name \".DS_Store\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-20T14:32:27.218622Z",
     "start_time": "2021-10-20T14:32:27.143386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 758 non-target images in the training set\n",
      "There are 861 target images in the training set\n"
     ]
    }
   ],
   "source": [
    "# Check functionality and number of images\n",
    "print('There are', len(os.listdir(train_open_dir)), 'non-target images in the training set')\n",
    "print('There are', len(os.listdir(train_vehicle_dir)), 'target images in the training set')\n",
    "\n",
    "# Expecting:\n",
    "# 758 non-target\n",
    "# 861 target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Validation-Test Split\n",
    "\n",
    "Only 100 images (50 of each class) were set aside as a testing/holdout set for final model evaluation in order to maximize the training set. An additional 100 images (50 of each class) have been set aside as a `validation` set to use during model training. The [split-folders](https://pypi.org/project/split-folders/) package provides an easy way to accomplish this and has methods for splitting either by a ratio or a fixed number. \n",
    "\n",
    "I actually split twice because I did not start with a `validation` set, opting instead to use the `validation_split` parameter in Keras's `ImageDataGenerator` class. My first split was as follows:\n",
    "```python\n",
    "\n",
    "\n",
    "However, this causes two serious data issues. First, it creates a non-random validation set because `validation_split=0.1` simply withholds the last 10% of images in the dataset. Since my images are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As new images were collected, they were added to a separate `new` folder, separate from the original `train` image set, then combined in a separate `full_combined` folder to be used for continued model training. \n",
    "\n",
    "The images are arranged in the following file structure:\n",
    "\n",
    "```\n",
    "└── input_images\n",
    "    ├── full_combined\n",
    "    ├── new\n",
    "    ├── test\n",
    "    ├── train\n",
    "    └── validation\n",
    "```\n",
    "Each folder of images contains 2 subfolders to designate image classes, as shown below with one example:\n",
    "```\n",
    "└── input_images\n",
    "    ├── full_combined\n",
    "    │    ├──open_bike_lane\n",
    "    │    └──vehicle_bike_lane\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution in training data\n",
    "total_images = len(open_images) + len(vehicle_images)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.bar(x=['Open', 'Vehicle'], height=[len(open_images)/total_images, \n",
    "                                                  len(vehicle_images)/total_images])\n",
    "ax.set_title('Class Distribution in Training Data', size=15)\n",
    "ax.set_ylabel('Percentage of Dataset', size=13)\n",
    "ax.set_xlabel('Image Class', size=13)\n",
    "ax.set_yticklabels([str(int(p*100))+'%' for p in ax.get_yticks()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Samples\n",
    "\n",
    "Below are samples of images from each class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
